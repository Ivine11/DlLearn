# 智能计算机应用基础

人工智能—机器学习—神经网络--深度学习

#### 机器学习

##### 符号说明

输入x， 真实y， 预测值$\hat{y}$， 模型函数H(x)， 激活函数G(x), 损失函数L(x)

标量：斜体小写， 向量：黑斜体小写，矩阵：黑斜体大写

### 神经网络

#### 神经网络结构

输入层+隐层+输出层

##### 线性模型：

$H_w(x) =\sum_{i=0}^{n} w_i x_i = \hat{w}^T x​$

损失：$L(\hat{w} )= \frac{1}{2} \sum_{j=1}^m (\hat{w}^Tx - y_j)^2​$

##### 图像分类：

比如要分十个类别，会得到一个10X1向量，表示属于每个类别的可能性

##### 感知机：二分类

$$H(x) = sign(\mathbf{w}^T x +b)​$$

$sign(x) = \begin {cases} +1 &x>=0\\ -1 & x<0\end{cases}$



#### 激活函数

##### sigmoid函数

1/（1+e^(-x))。能够把输入连续实值变换为0–1，

缺点：

产生输出的均值非0

饱和性问题和梯度消失现象

计算机进行指数运算速度慢

##### tanh函数

$$(e^x - e^{-x})/(e^x + e^{-x})$$

tanh 均值为0， 在输入很大或很小时，输出几乎平滑， 梯度小， 不利于权重更新

缺点：梯度消失

##### RELU函数：

max（0， x)

relu在x<0时不激活

##### Leakly Relu：

max(ax, x) , 0<a<1

##### ELU:

x>0, x

x<=0, $\alpha(e^x-1)$

#### 损失函数

##### 均方差损失函数

$$(y-y^x)^2/2$$

缺陷：均方差损失函数+sigmoid函数可能会出现梯度消失

##### 交叉熵损失函数：

![image-20220901142402054](/Users/pengruiying/Desktop/github/DlLearn/assets/image-20220901142402054.png)

二分类交叉损失函数：

![image-20220901142428059](/Users/pengruiying/Desktop/github/DlLearn/assets/image-20220901142428059.png)

##### 过拟合：

可能的原因：训练考虑的维度太多，使得拟合的函数很完美的接近训练数据集，但

##### 欠拟合：

训练考虑的维度太少

##### 防止过拟合：

参数范数惩罚，Bagging集成，Droupout，提前终止，数据集扩增

##### 参数范数惩罚：

![image-20220901143200286](/Users/pengruiying/Desktop/github/DlLearn/assets/image-20220901143200286.png)

![image-20220901143216276](/Users/pengruiying/Desktop/github/DlLearn/assets/image-20220901143216276.png)

在损失函数里加了权重矩阵wn，防止权重矩阵过大，降低网络复杂度

##### Dropout正则化：

在训练过程随机删除一些隐层单元，在计算时无视这些连接

##### k-折交叉验证：

不重复地每次取其中一份做验证集，其他k-1份做训练集

$MSE = \frac{1}{K} \sum_{i=1}^k MSE_i$

